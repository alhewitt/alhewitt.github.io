{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efficient-suspension",
   "metadata": {},
   "source": [
    "This project is taken from https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/ and is a started project to get experience working with deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "amazing-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-ready",
   "metadata": {},
   "source": [
    "The dataset to be used is medical information from Pima people and whether or not they had an onset of diabetes within five years. A 0 means no diabetes while 1 means an onset. All inputs are numerical.\n",
    "\n",
    "Input Variables (X):\n",
    "\n",
    "- Number of times pregnant\n",
    "- Plasma glucose concentration at 2 hours in an oral glucose tolerance test\n",
    "- Diastolic blood pressure (mm Hg)\n",
    "- Triceps skin fold thickness (mm)\n",
    "- 2-Hour serum insulin (mu U/ml)\n",
    "- Body mass index (weight in kg/(height in m)^2)\n",
    "- Diabetes pedigree function\n",
    "- Age (years)\n",
    "\n",
    "Output Variables (y):\n",
    "\n",
    "- Class variable (0 or 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "international-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "# split into input (X) and output (y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-royalty",
   "metadata": {},
   "source": [
    "In Keras **sequential** models are created. The first layer is the input layer which will have 8 dimensions for the 8 input variables. `input_dim=8`\n",
    "\n",
    "The number of layers to use can be found using trial and error but for this problem we will use a fully connected network structure with 3 layers. The first two layers will be rectified linear unit activation functions referred to as ReLU while the output layer will be a Sigmoid function. The Sigmoid function ensures the output is between 0 and 1 and easy to map to either a probability of class 1 or snap to a hard classification of either class with a default threshold of 0.5.\n",
    "\n",
    "All together, it looks like this:\n",
    "- The model expects rows of data with 8 variables (the `input_dim=8` argument)\n",
    "- The first hidden layer has 12 nodes and uses the ReLU activation function.\n",
    "- The second hidden layer has 8 nodes and uses the ReLU activation function.\n",
    "- The output layer has one node and uses the sigmoid activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "atmospheric-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-medium",
   "metadata": {},
   "source": [
    "**ReLU** is the positive part of its argument, so if $x\\leq0:$ return $0$, if $x>0:$ return $x$.\n",
    "\n",
    "The **sigmoid** function follows the equation $$\\frac{1}{1+e^{-x}}.$$ \n",
    "\n",
    "For small values ($x<-5$) the output is close to 0, while for large values ($x>5$) the output approaches 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "packed-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-employee",
   "metadata": {},
   "source": [
    "The **loss** function is used to evaluate a set of weights to map inputs to outputs in the dataset ($Y=\\sum(weight\\times input)+bias$). Loss should penalise bad predictions, so when the probability is low, you want a high value and when the probability is 1.0 you want the loss to be 0. In this problem we use binary cross entropy as our loss function as it is a binary clasification problem. The equation for this is\n",
    "$$H_p(q)=-\\frac{1}{N}\\sum^N_{i=1}y_i\\log(p(y_i))+(1-y_i)\\log(1-p(y_i))$$\n",
    "where $y_i$ would be 1 if there was an onset of diabetes and 0 if not, and $p(y_i)$ is the probability of an onset of diabetes. Since the loss function should be 0 for a perfect model and gets larger as the probability gets smaller. This is achieved by the negative log(probabity). Then the binary cross-entropy is the mean of each -log(probability).\n",
    "\n",
    "The **optimizer** searches through different weights and any other optional metrics we want to collect and report during training. In this case, \"adam\" is chosen because it automatically tunes itself and gives a good reasults for a wide variety of problems. Rather than having a single fixed learning rate (alpha) for all weight updates, adam has a different rate for each network weight which can be adapted as learning unfolds. Several parameters are used in adam:\n",
    "- $\\alpha$: Learning rate, the proportion that weights are updated by. Larger values lead to quicker learning\n",
    "- $\\beta1$: The exponential decay rate of the first moment estimates (the mean)\n",
    "- $\\beta2$: The exponential decay rate of the second moment estimates (the uncentred variance)\n",
    "- $\\epsilon$: A small number to prevent division-by-zero errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "yellow-screen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 781us/step - loss: 0.4131 - accuracy: 0.8047\n",
      "Accuracy: 80.47\n"
     ]
    }
   ],
   "source": [
    "# fit the keras model on the dataset\n",
    "model.fit(X, y, epochs=150, batch_size=10, verbose=0)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X, y)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "closed-mountain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0] => 1 (expected 1)\n",
      "[1.0, 85.0, 66.0, 29.0, 0.0, 26.6, 0.351, 31.0] => 0 (expected 0)\n",
      "[8.0, 183.0, 64.0, 0.0, 0.0, 23.3, 0.672, 32.0] => 1 (expected 1)\n",
      "[1.0, 89.0, 66.0, 23.0, 94.0, 28.1, 0.167, 21.0] => 0 (expected 0)\n",
      "[0.0, 137.0, 40.0, 35.0, 168.0, 43.1, 2.288, 33.0] => 1 (expected 1)\n"
     ]
    }
   ],
   "source": [
    "# make class predictions with the model\n",
    "predictions = (model.predict(X) > 0.5).astype(int)\n",
    "# summarize the first 5 cases\n",
    "for i in range(5):\n",
    "\tprint('%s => %d (expected %d)' % (X[i].tolist(), predictions[i], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-breeding",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enterprise",
   "language": "python",
   "name": "enterprise"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
